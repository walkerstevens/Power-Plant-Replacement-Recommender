{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data downloading and cleansing mostly focuses on the TMY 2020 data provided at https://nrel-pds-nsrdb.s3.amazonaws.com/v3/tmy/nsrdb_tmy-2020.h5 \n",
    "Not familiar with hdf5 data format, we started off from downloading the single file into a local storage of an Azure VM, and analyzes its applicability on our ML algorithms, which was positive.\n",
    "However, as each weather field of the dataset was compose of hourly data with 4.7 billion values - 7860 (1hr * 24 * 365 days) * 56K (US coordinates at 4km x 4km resolution) * value - aggregating it on a 16-gig VM turned out to be not feasible due to memory limitation.\n",
    "Decided to switch to PySpark on AWS EMR approach and leveraging multi-node distributed computing, processing time became significantly reduced, and we were able to extract the annual mean values of all 56K US coordinates - ghi, wind_speed and so on.\n",
    "\n",
    "# install dependancies \n",
    "# !pip install h5pyd\n",
    "# !pip install h5py\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install netCDF4 \n",
    "# !pip install scipy\n",
    "# !pip install s3fs\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9250c0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['air_temperature',\n",
       " 'alpha',\n",
       " 'aod',\n",
       " 'asymmetry',\n",
       " 'cld_opd_dcomp',\n",
       " 'cld_reff_dcomp',\n",
       " 'clearsky_dhi',\n",
       " 'clearsky_dni',\n",
       " 'clearsky_ghi',\n",
       " 'cloud_press_acha',\n",
       " 'cloud_type',\n",
       " 'coordinates',\n",
       " 'dew_point',\n",
       " 'dhi',\n",
       " 'dni',\n",
       " 'fill_flag',\n",
       " 'ghi',\n",
       " 'meta',\n",
       " 'ozone',\n",
       " 'relative_humidity',\n",
       " 'solar_zenith_angle',\n",
       " 'ssa',\n",
       " 'surface_albedo',\n",
       " 'surface_pressure',\n",
       " 'time_index',\n",
       " 'tmy_year',\n",
       " 'tmy_year_short',\n",
       " 'total_precipitable_water',\n",
       " 'wind_direction',\n",
       " 'wind_speed']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5pyd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.spatial import cKDTree\n",
    "import csv\n",
    "import s3fs\n",
    "\n",
    "# s3 = s3fs.S3FileSystem()\n",
    "\n",
    "# filename = 's3://nrel-pds-nsrdb/v3/tmy/nsrdb_tmy-2020.h5'\n",
    "# with s3.open(filename, 'rb') as f:\n",
    "#     nc_bytes = f.read()\n",
    "    \n",
    "# root = Dataset(f'inmemory.nc', memory=nc_bytes)    \n",
    "\n",
    "# Open the desired year of nsrdb data\n",
    "# server endpoint, username, password is found via a config file\n",
    "f = h5py.File('dataset/tmy2020.h5', 'r')\n",
    "list(f.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b88d4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14294</th>\n",
       "      <td>18.930000</td>\n",
       "      <td>-155.660004</td>\n",
       "      <td>66.263161</td>\n",
       "      <td>b'Hawaii'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14327</th>\n",
       "      <td>18.969999</td>\n",
       "      <td>-155.740005</td>\n",
       "      <td>46.846153</td>\n",
       "      <td>b'Hawaii'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14328</th>\n",
       "      <td>18.969999</td>\n",
       "      <td>-155.699997</td>\n",
       "      <td>64.631577</td>\n",
       "      <td>b'Hawaii'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14329</th>\n",
       "      <td>18.969999</td>\n",
       "      <td>-155.660004</td>\n",
       "      <td>211.360001</td>\n",
       "      <td>b'Hawaii'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14330</th>\n",
       "      <td>18.969999</td>\n",
       "      <td>-155.619995</td>\n",
       "      <td>93.428574</td>\n",
       "      <td>b'Hawaii'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14700</th>\n",
       "      <td>19.290001</td>\n",
       "      <td>-155.179993</td>\n",
       "      <td>320.359985</td>\n",
       "      <td>b'Hawaii'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14701</th>\n",
       "      <td>19.290001</td>\n",
       "      <td>-155.139999</td>\n",
       "      <td>234.440002</td>\n",
       "      <td>b'Hawaii'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14702</th>\n",
       "      <td>19.290001</td>\n",
       "      <td>-155.100006</td>\n",
       "      <td>103.250000</td>\n",
       "      <td>b'Hawaii'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14740</th>\n",
       "      <td>19.330000</td>\n",
       "      <td>-155.860001</td>\n",
       "      <td>416.299988</td>\n",
       "      <td>b'Hawaii'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14741</th>\n",
       "      <td>19.330000</td>\n",
       "      <td>-155.820007</td>\n",
       "      <td>1236.500000</td>\n",
       "      <td>b'Hawaii'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        latitude   longitude    elevation      state\n",
       "14294  18.930000 -155.660004    66.263161  b'Hawaii'\n",
       "14327  18.969999 -155.740005    46.846153  b'Hawaii'\n",
       "14328  18.969999 -155.699997    64.631577  b'Hawaii'\n",
       "14329  18.969999 -155.660004   211.360001  b'Hawaii'\n",
       "14330  18.969999 -155.619995    93.428574  b'Hawaii'\n",
       "...          ...         ...          ...        ...\n",
       "14700  19.290001 -155.179993   320.359985  b'Hawaii'\n",
       "14701  19.290001 -155.139999   234.440002  b'Hawaii'\n",
       "14702  19.290001 -155.100006   103.250000  b'Hawaii'\n",
       "14740  19.330000 -155.860001   416.299988  b'Hawaii'\n",
       "14741  19.330000 -155.820007  1236.500000  b'Hawaii'\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get coordinates in USA only (2018392 items -> 546219 items)\n",
    "meta = pd.DataFrame(f['meta'][...])\n",
    "\n",
    "USA = meta.loc[meta['country'] == b'United States'] # Note .h5 saves strings as bit-strings\n",
    "USA.head()\n",
    "\n",
    "# Keep 'elevation' and 'state' as well. They may be useful later.\n",
    "df_coord_usa = USA[['latitude', 'longitude', 'elevation', 'state']].copy()\n",
    "df_coord_usa.shape\n",
    "df_coord_usa.head(100)\n",
    "\n",
    "# dset = f['ghi']\n",
    "# data = dset[0][USA.index]  # full-resolution subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for convert h5 dataset into csv for USA coordinates only\n",
    "\n",
    "# meta = pd.DataFrame(f['meta'][...])\n",
    "# dset = f['ghi']\n",
    "\n",
    "# # Get coordinates in USA only (2018392 items -> 546219 items)\n",
    "# USA = meta.loc[meta['country'] == b'United States'] # Note .h5 saves strings as bit-strings\n",
    "# #USA.head()\n",
    "# #USA.index.tolist()\n",
    "\n",
    "# %time df = dset[0, USA.index.tolist()]\n",
    "# pd_df = pd.DataFrame(df)\n",
    "\n",
    "# # %time data = dset[...][USA.index]  # full-resolution subset\n",
    "# # df['ghi'] = data / dset.attrs['psm_scale_factor']\n",
    "# # df.shape\n",
    "\n",
    "# pd_df.head()\n",
    "# df.to_csv('dataset/ghi_all_us_coords.csv')\n",
    "\n",
    "# def save_to_csv_usa_only(dset_name):\n",
    "#     print(dset_name)\n",
    "#     df = pd.DataFrame(dset)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# df = save_to_csv_usa_only('ghi')\n",
    "# df.to_csv('dataset/us_powerplants_new_solar.csv')\n",
    "\n",
    "#header, table, df = data_wrangling('dataset/us_powerplants_orig_wind.csv')\n",
    "#df.to_csv('dataset/us_powerplants_new_wind.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e624ddf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NSRDB pixels in CA = 546219\n"
     ]
    }
   ],
   "source": [
    "meta = pd.DataFrame(f['meta'][...])\n",
    "ca_meta = meta.loc[meta['country'] == b'United States']\n",
    "ca_pos = ca_meta.index.values.copy()\n",
    "ca_slice = slice(ca_pos[0], ca_pos[-1] + 1)\n",
    "ca_pos -= ca_pos[0]\n",
    "down_size = 17520 * len(ca_pos) * 2 * 10**-6\n",
    "ca_meta.head()\n",
    "print('Number of NSRDB pixels in CA = {}'.format(len(ca_meta)))\n",
    "#print('Download size per year = {:.4f} MB'.format(down_size))\n",
    "\n",
    "ca_df = ca_meta[['latitude', 'longitude']].copy()\n",
    "ca_df.head(100)\n",
    "ghi = f['ghi'][:, ca_slice]\n",
    "\n",
    "# %time arr = f['ghi'][:, ca_slice] / f['ghi'].attrs['psm_scale_factor']\n",
    "#%time np.savetxt('dataset/ghi.csv', ghi, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4b731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
